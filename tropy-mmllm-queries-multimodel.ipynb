{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tropy Collection Analysis & Tagging Notebook\n",
    "\n",
    "# Tropy AI Analysis: Semantic Queries, Clustering, and Tagging\n",
    "\n",
    "### **Goal**\n",
    "This notebook is the interactive analysis and organization component of the AI-powered research workflow. It uses the vector embeddings and AI-generated summaries created by the `tropy-mmllm-analysis-multimodel.ipynb` notebook to enable powerful new ways of interacting with your collection.\n",
    "\n",
    "### **Features**\n",
    "- **Semantic Search:** NLP queries goes beyond keywords.\n",
    "- **Automated Tagging:** Find relevant items based on your query and programmatically apply tags in Tropy.\n",
    "- **Theme Discovery (Clustering):** Automatically identify and group semantically similar documents to uncover hidden themes in your collection.\n",
    "- **Metadata Enhancement:** Use AI-suggested titles from your summaries to automatically update the metadata of your Tropy items.\n",
    "\n",
    "### **How to Use This Notebook**\n",
    "1.  **Prerequisite:** Ensure you have already run the `tropy-mmllm-analysis-multimodel.ipynb` notebook to create the `tropy_embeddings.json` and `output/item_summaries.json` files.\n",
    "2.  **Execution Order:** Run the code cells sequentially from top to bottom.\n",
    "    *   **Cell 1** loads all necessary libraries and your embeddings data.\n",
    "    *   **Cell 2** defines all the helper functions for searching, tagging, and analysis.\n",
    "    *   **Cell 3** is the main interactive menu where you will run your analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import Required Libraries \n",
    "import json, requests, os, time\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, HTML\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import Optional, List, Dict, Any\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ All dependencies imported successfully\")\n",
    "print(\"üìù Note: Using numpy-based similarity search (perfect for collections under 10k photos)\")\n",
    "print(\"   For larger collections, consider ChromaDB or FAISS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configure API Keys and Select Model (New - matching analysis notebook)\n",
    "print(\"üîß Setting up AI Model Providers...\\n\")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Check and configure API keys\n",
    "providers_available = []\n",
    "\n",
    "# Helper function to add/check API key\n",
    "def check_api_key(provider_name, env_var, display_name):\n",
    "    if os.getenv(env_var):\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"üìù {provider_name} API key not found\")\n",
    "        add_key = input(f\"   Add {provider_name} API key? (y/n) [n]: \").strip().lower()\n",
    "        if add_key == 'y':\n",
    "            api_key = input(f\"   Paste your {provider_name} API key: \").strip()\n",
    "            if api_key:\n",
    "                with open('.env', 'a') as f:\n",
    "                    f.write(f\"\\n{env_var}={api_key}\\n\")\n",
    "                load_dotenv()\n",
    "                print(f\"   ‚úÖ Added {provider_name} key to .env\")\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "# Check each provider\n",
    "print(\"üìä Checking available providers:\\n\")\n",
    "\n",
    "# Google Gemini\n",
    "if check_api_key(\"Google\", \"GOOGLE_API_KEY\", \"Gemini\"):\n",
    "    try:\n",
    "        import google.generativeai as genai\n",
    "        genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "        providers_available.append((\"gemini\", \"Google Gemini (gemini-1.5-flash)\"))\n",
    "    except ImportError:\n",
    "        print(\"   ‚ö†Ô∏è  Google AI library not installed. Run: pip install google-generativeai\")\n",
    "\n",
    "# OpenAI\n",
    "if check_api_key(\"OpenAI\", \"OPENAI_API_KEY\", \"GPT-4\"):\n",
    "    try:\n",
    "        import openai\n",
    "        providers_available.append((\"openai\", \"OpenAI (gpt-4o-mini)\"))\n",
    "    except ImportError:\n",
    "        print(\"   ‚ö†Ô∏è  OpenAI library not installed. Run: pip install openai\")\n",
    "\n",
    "# Anthropic\n",
    "if check_api_key(\"Anthropic\", \"ANTHROPIC_API_KEY\", \"Claude\"):\n",
    "    try:\n",
    "        import anthropic\n",
    "        providers_available.append((\"claude\", \"Anthropic Claude (claude-3-haiku)\"))\n",
    "    except ImportError:\n",
    "        print(\"   ‚ö†Ô∏è  Anthropic library not installed. Run: pip install anthropic\")\n",
    "\n",
    "# Select model\n",
    "print(f\"\\nüìã Available models: {len(providers_available)}\")\n",
    "\n",
    "if len(providers_available) == 0:\n",
    "    print(\"‚ùå No models available. Please configure at least one API key.\")\n",
    "    raise ValueError(\"No API keys configured\")\n",
    "elif len(providers_available) == 1:\n",
    "    selected = providers_available[0]\n",
    "    selected_model = selected[0]\n",
    "    print(f\"‚úÖ Using: {selected[1]}\")\n",
    "else:\n",
    "    print(\"\\nüéØ Select your model:\")\n",
    "    for i, (key, name) in enumerate(providers_available):\n",
    "        print(f\"   {i+1}. {name}\")\n",
    "    \n",
    "    choice = input(f\"\\nYour choice (1-{len(providers_available)}) [1]: \").strip() or \"1\"\n",
    "    try:\n",
    "        idx = int(choice) - 1\n",
    "        selected = providers_available[idx]\n",
    "        selected_model = selected[0]\n",
    "        print(f\"\\n‚úÖ Selected: {selected[1]}\")\n",
    "    except:\n",
    "        selected = providers_available[0]\n",
    "        selected_model = selected[0]\n",
    "        print(f\"\\n‚úÖ Defaulting to: {selected[1]}\")\n",
    "\n",
    "print(\"\\n‚ú® Model selection complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Configuration Settings (New - matching analysis notebook)\n",
    "class Config:\n",
    "    \"\"\"Central configuration for the queries workflow.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_provider):\n",
    "        # API Settings\n",
    "        self.TROPY_PROJECT_API = 'http://127.0.0.1:2019/project'\n",
    "        self.TROPY_API_URL = 'http://127.0.0.1:2019'\n",
    "        \n",
    "        # Dynamic model configuration based on selection\n",
    "        if model_provider == 'gemini':\n",
    "            self.EMBEDDING_MODEL = 'models/embedding-001'\n",
    "        elif model_provider == 'openai':\n",
    "            self.EMBEDDING_MODEL = 'text-embedding-3-small'\n",
    "        elif model_provider == 'claude':\n",
    "            self.EMBEDDING_MODEL = None  # Claude doesn't have native embeddings\n",
    "        \n",
    "        # File paths\n",
    "        self.EMBEDDINGS_FILE = f\"tropy_embeddings_{model_provider}.json\"\n",
    "        self.ITEM_SUMMARIES_FILE = f\"output/item_summaries_{model_provider}.json\"\n",
    "        \n",
    "        # For backward compatibility with gemini\n",
    "        if model_provider == 'gemini':\n",
    "            # Also check for non-model-specific files\n",
    "            if os.path.exists(\"tropy_embeddings.json\"):\n",
    "                self.EMBEDDINGS_FILE = \"tropy_embeddings.json\"\n",
    "            if os.path.exists(\"output/item_summaries.json\"):\n",
    "                self.ITEM_SUMMARIES_FILE = \"output/item_summaries.json\"\n",
    "\n",
    "config = Config(selected_model)\n",
    "print(f\"‚úÖ Configuration initialized for {selected_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Model Adapters for Embeddings (New)\n",
    "class EmbeddingAdapter:\n",
    "    \"\"\"Base adapter interface for embeddings\"\"\"\n",
    "    def generate_query_embedding(self, query: str):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class GeminiEmbeddingAdapter(EmbeddingAdapter):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        import google.generativeai as genai\n",
    "        self.genai = genai\n",
    "    \n",
    "    def generate_query_embedding(self, query: str):\n",
    "        try:\n",
    "            result = self.genai.embed_content(\n",
    "                model=self.config.EMBEDDING_MODEL, \n",
    "                content=query, \n",
    "                task_type=\"RETRIEVAL_QUERY\"\n",
    "            )\n",
    "            return result['embedding']\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating Gemini embedding: {e}\")\n",
    "            return None\n",
    "\n",
    "class OpenAIEmbeddingAdapter(EmbeddingAdapter):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        from openai import OpenAI\n",
    "        self.client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    \n",
    "    def generate_query_embedding(self, query: str):\n",
    "        try:\n",
    "            response = self.client.embeddings.create(\n",
    "                model=self.config.EMBEDDING_MODEL,\n",
    "                input=query\n",
    "            )\n",
    "            return response.data[0].embedding\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating OpenAI embedding: {e}\")\n",
    "            return None\n",
    "\n",
    "class ClaudeEmbeddingAdapter(EmbeddingAdapter):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        # Claude doesn't have embeddings, so we'll use OpenAI if available\n",
    "        if os.getenv(\"OPENAI_API_KEY\"):\n",
    "            from openai import OpenAI\n",
    "            self.client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "            self.use_openai = True\n",
    "        else:\n",
    "            self.use_openai = False\n",
    "            logger.warning(\"Claude doesn't provide embeddings and OpenAI key not available\")\n",
    "    \n",
    "    def generate_query_embedding(self, query: str):\n",
    "        if self.use_openai:\n",
    "            try:\n",
    "                response = self.client.embeddings.create(\n",
    "                    model=\"text-embedding-3-small\",\n",
    "                    input=query\n",
    "                )\n",
    "                return response.data[0].embedding\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error generating embedding: {e}\")\n",
    "                return None\n",
    "        else:\n",
    "            logger.error(\"No embedding model available for Claude\")\n",
    "            return None\n",
    "\n",
    "# Factory function\n",
    "def get_embedding_adapter(config, model_provider):\n",
    "    if model_provider == 'gemini':\n",
    "        return GeminiEmbeddingAdapter(config)\n",
    "    elif model_provider == 'openai':\n",
    "        return OpenAIEmbeddingAdapter(config)\n",
    "    elif model_provider == 'claude':\n",
    "        return ClaudeEmbeddingAdapter(config)\n",
    "    else:\n",
    "        return GeminiEmbeddingAdapter(config)\n",
    "\n",
    "# Initialize adapter\n",
    "embedding_adapter = get_embedding_adapter(config, selected_model)\n",
    "print(f\"‚úÖ Embedding adapter configured for {selected_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Load Data and Build Search Index (Updated - Smart File Discovery)\n",
    "\n",
    "# Function to find embeddings file\n",
    "def find_embeddings_file():\n",
    "    \"\"\"Try to find any embeddings file, regardless of model.\"\"\"\n",
    "    possible_files = [\n",
    "        # Model-specific files\n",
    "        f\"tropy_embeddings_{selected_model}.json\",\n",
    "        f\"output/tropy_embeddings_{selected_model}.json\",\n",
    "        \n",
    "        # Generic files\n",
    "        \"tropy_embeddings.json\",\n",
    "        \"output/tropy_embeddings.json\",\n",
    "        \n",
    "        # All possible model files\n",
    "        \"tropy_embeddings_gemini.json\",\n",
    "        \"tropy_embeddings_openai.json\", \n",
    "        \"tropy_embeddings_claude.json\",\n",
    "        \"output/tropy_embeddings_gemini.json\",\n",
    "        \"output/tropy_embeddings_openai.json\",\n",
    "        \"output/tropy_embeddings_claude.json\",\n",
    "    ]\n",
    "    \n",
    "    for file_path in possible_files:\n",
    "        if os.path.exists(file_path):\n",
    "            return file_path\n",
    "    \n",
    "    # If nothing found, scan current directory for any embeddings file\n",
    "    for file in os.listdir('.'):\n",
    "        if 'embeddings' in file and file.endswith('.json'):\n",
    "            return file\n",
    "    \n",
    "    # Check output directory\n",
    "    if os.path.exists('output'):\n",
    "        for file in os.listdir('output'):\n",
    "            if 'embeddings' in file and file.endswith('.json'):\n",
    "                return os.path.join('output', file)\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Function to find summaries file\n",
    "def find_summaries_file():\n",
    "    \"\"\"Try to find any summaries file.\"\"\"\n",
    "    possible_files = [\n",
    "        # Model-specific files\n",
    "        f\"output/item_summaries_{selected_model}.json\",\n",
    "        f\"item_summaries_{selected_model}.json\",\n",
    "        \n",
    "        # Generic files\n",
    "        \"output/item_summaries.json\",\n",
    "        \"item_summaries.json\",\n",
    "        \n",
    "        # All possible model files\n",
    "        \"output/item_summaries_gemini.json\",\n",
    "        \"output/item_summaries_openai.json\",\n",
    "        \"output/item_summaries_claude.json\",\n",
    "        \"item_summaries_gemini.json\",\n",
    "        \"item_summaries_openai.json\",\n",
    "        \"item_summaries_claude.json\",\n",
    "    ]\n",
    "    \n",
    "    for file_path in possible_files:\n",
    "        if os.path.exists(file_path):\n",
    "            return file_path\n",
    "    \n",
    "    # Scan directories\n",
    "    for directory in ['.', 'output']:\n",
    "        if os.path.exists(directory):\n",
    "            for file in os.listdir(directory):\n",
    "                if 'summaries' in file and file.endswith('.json'):\n",
    "                    return os.path.join(directory, file) if directory != '.' else file\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Find actual files\n",
    "embedding_file = find_embeddings_file()\n",
    "summaries_file = find_summaries_file()\n",
    "\n",
    "print(\"üîç File Discovery Results:\")\n",
    "print(f\"   Looking for embeddings with model: {selected_model}\")\n",
    "print(f\"   Found embeddings: {embedding_file or 'NOT FOUND'}\")\n",
    "print(f\"   Found summaries: {summaries_file or 'NOT FOUND'}\")\n",
    "\n",
    "# Load Embeddings\n",
    "embeddings_records = []\n",
    "embedding_matrix = None\n",
    "\n",
    "if embedding_file:\n",
    "    try:\n",
    "        with open(embedding_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Handle different file formats\n",
    "        if isinstance(data, dict) and 'photo_embeddings' in data:\n",
    "            embeddings_records = data['photo_embeddings']\n",
    "        elif isinstance(data, list):\n",
    "            embeddings_records = data\n",
    "        else:\n",
    "            raise ValueError(\"Invalid embeddings file format\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ Successfully loaded {len(embeddings_records)} records from {embedding_file}\")\n",
    "        \n",
    "        # Check embedding dimensions match selected model\n",
    "        if embeddings_records:\n",
    "            embedding_dim = len(embeddings_records[0]['embedding'])\n",
    "            expected_dims = {\n",
    "                'gemini': 768,  # Gemini embedding dimension\n",
    "                'openai': 1536,  # OpenAI text-embedding-3-small dimension\n",
    "                'claude': 1536   # Would use OpenAI fallback\n",
    "            }\n",
    "            \n",
    "            if selected_model in expected_dims and embedding_dim != expected_dims[selected_model]:\n",
    "                print(f\"\\n‚ö†Ô∏è  Warning: Embedding dimensions ({embedding_dim}) don't match expected for {selected_model} ({expected_dims[selected_model]})\")\n",
    "                print(f\"   This suggests embeddings were created with a different model.\")\n",
    "                print(f\"   Search will still work, but consider regenerating embeddings with {selected_model} for optimal results.\")\n",
    "        \n",
    "        # Build numpy matrix for similarity search\n",
    "        embedding_matrix = np.array([record['embedding'] for record in embeddings_records]).astype('float32')\n",
    "        print(f\"‚úÖ Created embedding matrix with shape: {embedding_matrix.shape}\")\n",
    "        print(f\"üìä Using numpy-based similarity search (efficient for {len(embeddings_records)} documents)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå ERROR loading embeddings file '{embedding_file}': {e}\")\n",
    "        embeddings_records = []\n",
    "        embedding_matrix = None\n",
    "else:\n",
    "    print(f\"\\n‚ùå No embeddings file found!\")\n",
    "    print(\"\\nüìù To fix this:\")\n",
    "    print(\"   1. Run the analysis notebook first to generate embeddings\")\n",
    "    print(\"   2. Make sure the embeddings file is in the current directory or 'output' folder\")\n",
    "    print(\"   3. The file should be named like 'tropy_embeddings.json' or 'tropy_embeddings_[model].json'\")\n",
    "\n",
    "# Load Item Summaries\n",
    "item_summaries = []\n",
    "\n",
    "if summaries_file:\n",
    "    try:\n",
    "        with open(summaries_file, 'r') as f:\n",
    "            item_summaries = json.load(f)\n",
    "        print(f\"\\n‚úÖ Successfully loaded {len(item_summaries)} AI-generated item summaries.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ö†Ô∏è Could not load summaries file: {e}\")\n",
    "        item_summaries = []\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è No summaries file found. Metadata enhancement will be disabled.\")\n",
    "\n",
    "# Summary\n",
    "if embeddings_records:\n",
    "    print(f\"\\nüéâ Ready for analysis with {len(embeddings_records)} embeddings!\")\n",
    "    if 'gemini' in embedding_file.lower() and selected_model != 'gemini':\n",
    "        print(f\"\\nüí° Note: You're using {selected_model} for queries but embeddings were created with Gemini.\")\n",
    "        print(\"   This works fine, but for consistency you might want to:\")\n",
    "        print(f\"   - Either: Regenerate embeddings using {selected_model} in the analysis notebook\")\n",
    "        print(\"   - Or: Restart and select Gemini when running this notebook\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Cannot proceed without embeddings. Please run the analysis notebook first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Similarity Search Functions \n",
    "\n",
    "def numpy_similarity_search(query_vector, embedding_matrix, top_k=10):\n",
    "    \"\"\"\n",
    "    Perform similarity search using numpy (efficient for small to medium collections).\n",
    "    For collections over 10k documents, consider ChromaDB or FAISS.\n",
    "    \"\"\"\n",
    "    # Normalize query vector\n",
    "    query_norm = query_vector / np.linalg.norm(query_vector)\n",
    "    \n",
    "    # Normalize all embeddings (for cosine similarity)\n",
    "    norms = np.linalg.norm(embedding_matrix, axis=1)\n",
    "    normalized_embeddings = embedding_matrix / norms[:, np.newaxis]\n",
    "    \n",
    "    # Calculate similarities\n",
    "    similarities = np.dot(normalized_embeddings, query_norm)\n",
    "    \n",
    "    # Get top k indices\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    \n",
    "    # Convert similarities to distances (for compatibility)\n",
    "    distances = 1 - similarities[top_indices]\n",
    "    \n",
    "    return distances, top_indices\n",
    "\n",
    "print(\"‚úÖ Similarity search functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Enhanced Search with Better Previews and Selective Tagging\n",
    "\n",
    "def search_and_tag_workflow():\n",
    "    \"\"\"Interactive search with longer previews and selective tagging.\"\"\"\n",
    "    query = input(\"\\nEnter your search query: \").strip()\n",
    "    if not query:\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nüîé Searching for: '{query}'...\")\n",
    "    \n",
    "    # Lower default threshold\n",
    "    DEFAULT_THRESHOLD = 0.3\n",
    "    \n",
    "    # Generate query embedding\n",
    "    query_embedding = embedding_adapter.generate_query_embedding(query)\n",
    "    if query_embedding is None:\n",
    "        print(\"‚ùå Failed to generate query embedding\")\n",
    "        return\n",
    "    \n",
    "    query_vector = np.array(query_embedding).astype('float32')\n",
    "    \n",
    "    # Search ALL documents\n",
    "    distances, indices = numpy_similarity_search(query_vector, embedding_matrix, top_k=len(embeddings_records))\n",
    "    similarities = 1 - distances\n",
    "    \n",
    "    # Show top similarities for debugging\n",
    "    print(f\"\\nüìä Top 5 similarity scores:\")\n",
    "    for i in range(min(5, len(similarities))):\n",
    "        print(f\"   {i+1}. {similarities[i]:.3f} ({similarities[i]*100:.1f}%)\")\n",
    "    \n",
    "    # Try different thresholds if needed\n",
    "    threshold = DEFAULT_THRESHOLD\n",
    "    relevant_mask = similarities >= threshold\n",
    "    num_results = np.sum(relevant_mask)\n",
    "    \n",
    "    # If too few results, show what we have\n",
    "    if num_results == 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  No results above {threshold:.0%} threshold.\")\n",
    "        print(\"üìâ Showing top 10 results regardless of threshold:\\n\")\n",
    "        \n",
    "        # Just take top 10\n",
    "        relevant_indices = indices[:10]\n",
    "        relevant_similarities = similarities[:10]\n",
    "        threshold = 0\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ Found {num_results} results above {threshold:.0%} threshold\")\n",
    "        \n",
    "        # Extract initial results based on threshold\n",
    "        relevant_indices = indices[relevant_mask]\n",
    "        relevant_similarities = similarities[relevant_mask]\n",
    "        \n",
    "        # Ask if user wants to adjust threshold\n",
    "        print(\"\\nüéöÔ∏è  Threshold Options:\")\n",
    "        print(f\"   1. Continue with current results ({num_results} items)\")\n",
    "        print(\"   2. Show more results (lower threshold)\")\n",
    "        print(\"   3. Show fewer results (higher threshold)\")\n",
    "        print(\"   4. Show top N results regardless of threshold\")\n",
    "        \n",
    "        adjust = input(\"\\nYour choice (1-4) [1]: \").strip() or \"1\"\n",
    "        \n",
    "        if adjust == \"2\":\n",
    "            new_threshold = float(input(f\"Enter lower threshold (0.0-{threshold:.1f}) [0.2]: \").strip() or \"0.2\")\n",
    "            threshold = new_threshold\n",
    "            relevant_mask = similarities >= threshold\n",
    "            relevant_indices = indices[relevant_mask]\n",
    "            relevant_similarities = similarities[relevant_mask]\n",
    "        elif adjust == \"3\":\n",
    "            new_threshold = float(input(f\"Enter higher threshold ({threshold:.1f}-1.0) [0.5]: \").strip() or \"0.5\")\n",
    "            threshold = new_threshold\n",
    "            relevant_mask = similarities >= threshold\n",
    "            relevant_indices = indices[relevant_mask]\n",
    "            relevant_similarities = similarities[relevant_mask]\n",
    "        elif adjust == \"4\":\n",
    "            n = int(input(\"How many top results to show? [20]: \").strip() or \"20\")\n",
    "            relevant_indices = indices[:n]\n",
    "            relevant_similarities = similarities[:n]\n",
    "            threshold = 0\n",
    "        # If adjust == \"1\", we already have relevant_indices set above\n",
    "    \n",
    "    # Get results\n",
    "    results = []\n",
    "    for idx, sim in zip(relevant_indices, relevant_similarities):\n",
    "        result = embeddings_records[idx].copy()\n",
    "        result['similarity'] = sim\n",
    "        results.append(result)\n",
    "    \n",
    "    if not results:\n",
    "        print(\"\\n‚ùå No results to display\")\n",
    "        return\n",
    "    \n",
    "    # Group by items\n",
    "    item_to_photos = {}\n",
    "    for result in results:\n",
    "        item_id = result.get('item_id')\n",
    "        item_title = result.get('item_title', 'Untitled')\n",
    "        \n",
    "        if item_id not in item_to_photos:\n",
    "            item_to_photos[item_id] = {\n",
    "                'title': item_title,\n",
    "                'photos': [],\n",
    "                'max_similarity': 0\n",
    "            }\n",
    "        \n",
    "        item_to_photos[item_id]['photos'].append(result)\n",
    "        item_to_photos[item_id]['max_similarity'] = max(\n",
    "            item_to_photos[item_id]['max_similarity'], \n",
    "            result['similarity']\n",
    "        )\n",
    "    \n",
    "    # Display results with longer previews\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä SEARCH RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"üîç Query: '{query}'\")\n",
    "    print(f\"üìä Threshold: {threshold:.0%} similarity\")\n",
    "    print(f\"üì∑ Photos found: {len(results)}\")\n",
    "    print(f\"üìö Unique items: {len(item_to_photos)}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Store items in a list for selection\n",
    "    items_list = list(item_to_photos.items())\n",
    "    \n",
    "    # Show items with longer previews\n",
    "    print(\"\\nüìã Items found:\")\n",
    "    for i, (item_id, data) in enumerate(items_list):\n",
    "        # Color code by similarity\n",
    "        sim = data['max_similarity']\n",
    "        if sim >= 0.5:\n",
    "            icon = \"üü¢\"\n",
    "        elif sim >= 0.3:\n",
    "            icon = \"üü°\"\n",
    "        else:\n",
    "            icon = \"üî¥\"\n",
    "        \n",
    "        print(f\"\\n{icon} {i+1}. {data['title']} (ID: {item_id})\")\n",
    "        print(f\"      Best match: {sim:.1%} | Photos: {len(data['photos'])}\")\n",
    "        \n",
    "        # Show longer preview (up to 400 chars)\n",
    "        best_photo = max(data['photos'], key=lambda x: x['similarity'])\n",
    "        summary = best_photo.get('summary', 'No summary')\n",
    "        preview_length = min(len(summary), 400)  # Show up to 400 characters\n",
    "        print(f\"      Preview: \\\"{summary[:preview_length]}{'...' if len(summary) > preview_length else ''}\\\"\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    \n",
    "    # View options\n",
    "    print(\"\\nüìÑ View Options:\")\n",
    "    print(\"   1. Continue to tagging\")\n",
    "    print(\"   2. Show full summaries for specific items\")\n",
    "    print(\"   3. Show all full summaries\")\n",
    "    print(\"   4. Cancel\")\n",
    "    \n",
    "    view_choice = input(\"\\nYour choice (1-4) [1]: \").strip() or \"1\"\n",
    "    \n",
    "    if view_choice == \"2\":\n",
    "        item_nums = input(\"Enter item numbers to view (e.g., 1,3,5): \").strip()\n",
    "        try:\n",
    "            nums = [int(x.strip()) - 1 for x in item_nums.split(',')]\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            for num in nums:\n",
    "                if 0 <= num < len(items_list):\n",
    "                    item_id, data = items_list[num]\n",
    "                    print(f\"\\nüìÑ FULL DETAILS - Item {num+1}: {data['title']}\")\n",
    "                    print(\"=\"*70)\n",
    "                    for j, photo in enumerate(data['photos']):\n",
    "                        print(f\"\\nPhoto {j+1} (Similarity: {photo['similarity']:.1%}):\")\n",
    "                        print(f\"Full summary: {photo.get('summary', 'No summary')}\")\n",
    "                        print(\"-\"*50)\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "        except:\n",
    "            print(\"Invalid input\")\n",
    "    \n",
    "    elif view_choice == \"3\":\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üìÑ ALL FULL SUMMARIES\")\n",
    "        print(\"=\"*70)\n",
    "        for i, (item_id, data) in enumerate(items_list[:10]):  # Limit to first 10\n",
    "            print(f\"\\n{i+1}. {data['title']}\")\n",
    "            best_photo = max(data['photos'], key=lambda x: x['similarity'])\n",
    "            print(f\"Full summary: {best_photo.get('summary', 'No summary')}\")\n",
    "            print(\"-\"*70)\n",
    "        if len(items_list) > 10:\n",
    "            print(f\"\\n... showing first 10 of {len(items_list)} items\")\n",
    "    \n",
    "    elif view_choice == \"4\":\n",
    "        return\n",
    "    \n",
    "    # Tagging with selection options\n",
    "    if len(items_list) == 0:\n",
    "        print(\"No items to tag.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nüè∑Ô∏è  TAGGING OPTIONS\")\n",
    "    print(f\"   Total items available: {len(items_list)}\")\n",
    "    print(\"\\n   1. Tag ALL items\")\n",
    "    print(\"   2. Tag specific items by number\")\n",
    "    print(\"   3. Tag only high relevance items (‚â•50%)\")\n",
    "    print(\"   4. Skip tagging\")\n",
    "    \n",
    "    tag_choice = input(\"\\nYour choice (1-4) [1]: \").strip() or \"1\"\n",
    "    \n",
    "    if tag_choice == \"4\":\n",
    "        return\n",
    "    \n",
    "    # Determine items to tag\n",
    "    items_to_tag = []\n",
    "    \n",
    "    if tag_choice == \"1\":\n",
    "        items_to_tag = [item_id for item_id, _ in items_list]\n",
    "        print(f\"\\n‚úÖ Will tag ALL {len(items_to_tag)} items\")\n",
    "        \n",
    "    elif tag_choice == \"2\":\n",
    "        # SELECT SPECIFIC ITEMS BY NUMBER\n",
    "        item_nums = input(\"Enter item numbers to tag (e.g., 1,2,3,4): \").strip()\n",
    "        try:\n",
    "            selected_nums = [int(x.strip()) - 1 for x in item_nums.split(',')]\n",
    "            items_to_tag = []\n",
    "            selected_titles = []\n",
    "            \n",
    "            for num in selected_nums:\n",
    "                if 0 <= num < len(items_list):\n",
    "                    item_id, data = items_list[num]\n",
    "                    items_to_tag.append(item_id)\n",
    "                    selected_titles.append(f\"{num+1}. {data['title']}\")\n",
    "            \n",
    "            print(f\"\\n‚úÖ Will tag {len(items_to_tag)} selected items:\")\n",
    "            for title in selected_titles[:5]:\n",
    "                print(f\"   {title}\")\n",
    "            if len(selected_titles) > 5:\n",
    "                print(f\"   ... and {len(selected_titles) - 5} more\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Invalid input: {e}\")\n",
    "            return\n",
    "            \n",
    "    elif tag_choice == \"3\":\n",
    "        items_to_tag = [item_id for item_id, data in items_list if data['max_similarity'] >= 0.5]\n",
    "        print(f\"\\n‚úÖ Will tag {len(items_to_tag)} high-relevance items\")\n",
    "    \n",
    "    if not items_to_tag:\n",
    "        print(\"‚ùå No items selected for tagging\")\n",
    "        return\n",
    "    \n",
    "    # Apply tag\n",
    "    tag_name = input(\"\\nEnter tag name: \").strip()\n",
    "    if tag_name:\n",
    "        add_tag_to_items(items_to_tag, tag_name)\n",
    "    else:\n",
    "        print(\"‚ùå No tag name provided\")\n",
    "\n",
    "print(\"‚úÖ Enhanced search with selective tagging ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: API Client (New)\n",
    "class TropyAPIClient:\n",
    "    \"\"\"Handles Tropy API interactions.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.session = requests.Session()\n",
    "    \n",
    "    def fetch_all_tags(self):\n",
    "        try:\n",
    "            response = self.session.get(f\"{self.config.TROPY_PROJECT_API}/tags\")\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to fetch tags: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def create_tag(self, tag_name, color=\"#4A90E2\"):\n",
    "        try:\n",
    "            payload = {'@type': 'Tag', 'title': tag_name, 'color': color}\n",
    "            response = self.session.post(f\"{self.config.TROPY_PROJECT_API}/tags\", json=payload)\n",
    "            response.raise_for_status()\n",
    "            return response.json().get('id')\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to create tag: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def add_tag_to_item(self, item_id, tag_id):\n",
    "        try:\n",
    "            response = self.session.post(\n",
    "                f\"{self.config.TROPY_PROJECT_API}/items/{item_id}/tags\", \n",
    "                json={\"tag\": tag_id}\n",
    "            )\n",
    "            return response.status_code in [200, 201, 204]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to add tag to item {item_id}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def update_item_title(self, item_id, new_title):\n",
    "        try:\n",
    "            payload = {\"http://purl.org/dc/elements/1.1/title\": new_title}\n",
    "            response = self.session.post(\n",
    "                f\"{self.config.TROPY_PROJECT_API}/items/{item_id}/data\", \n",
    "                json=payload\n",
    "            )\n",
    "            return response.status_code in [200, 201, 204]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to update item title: {e}\")\n",
    "            return False\n",
    "\n",
    "# Initialize API client\n",
    "api_client = TropyAPIClient(config)\n",
    "print(\"‚úÖ API client initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Fixed Tagging Function \n",
    "\n",
    "def add_tag_to_items(item_ids, tag_name, color=\"#4A90E2\"):\n",
    "    \"\"\"Fixed version that properly handles Tropy API.\"\"\"\n",
    "    if not item_ids: \n",
    "        return\n",
    "    \n",
    "    unique_ids = list(set(item_ids))\n",
    "    print(f\"\\nüè∑Ô∏è Preparing to apply tag '{tag_name}' to {len(unique_ids)} unique items...\")\n",
    "    \n",
    "    try:\n",
    "        # First, let's check what tags endpoint returns\n",
    "        test_response = api_client.session.get(f\"{config.TROPY_PROJECT_API}/tags\")\n",
    "        \n",
    "        if test_response.status_code == 404:\n",
    "            print(\"‚ö†Ô∏è  Tags endpoint not found. Trying alternative endpoints...\")\n",
    "            # Try without /project\n",
    "            test_response = api_client.session.get(f\"{config.TROPY_API_URL}/tags\")\n",
    "        \n",
    "        existing_tags = []\n",
    "        if test_response.status_code == 200:\n",
    "            existing_tags = test_response.json()\n",
    "        \n",
    "        # Find existing tag\n",
    "        tag_id = None\n",
    "        for tag in existing_tags:\n",
    "            # Check multiple possible field names\n",
    "            tag_title = tag.get('title') or tag.get('name') or tag.get('value', '')\n",
    "            if tag_title == tag_name:\n",
    "                tag_id = tag.get('id')\n",
    "                print(f\"‚úÖ Found existing tag '{tag_name}' (ID: {tag_id})\")\n",
    "                break\n",
    "        \n",
    "        # Create tag if needed\n",
    "        if not tag_id:\n",
    "            print(f\"Creating new tag '{tag_name}'...\")\n",
    "            \n",
    "            # Try multiple endpoint and payload combinations\n",
    "            endpoints_to_try = [\n",
    "                f\"{config.TROPY_PROJECT_API}/tags\",\n",
    "                f\"{config.TROPY_API_URL}/tags\",\n",
    "                f\"{config.TROPY_API_URL}/project/tags\"\n",
    "            ]\n",
    "            \n",
    "            payloads_to_try = [\n",
    "                {\"name\": tag_name, \"color\": color},\n",
    "                {\"title\": tag_name, \"color\": color},\n",
    "                {\"value\": tag_name, \"color\": color},\n",
    "                {\"tag\": {\"name\": tag_name, \"color\": color}},\n",
    "            ]\n",
    "            \n",
    "            created = False\n",
    "            for endpoint in endpoints_to_try:\n",
    "                if created:\n",
    "                    break\n",
    "                for payload in payloads_to_try:\n",
    "                    try:\n",
    "                        response = api_client.session.post(\n",
    "                            endpoint,\n",
    "                            json=payload,\n",
    "                            headers={'Content-Type': 'application/json'}\n",
    "                        )\n",
    "                        \n",
    "                        if response.status_code in [200, 201]:\n",
    "                            result = response.json()\n",
    "                            tag_id = result.get('id') or result.get('tag', {}).get('id')\n",
    "                            if tag_id:\n",
    "                                print(f\"‚úÖ Successfully created tag with ID: {tag_id}\")\n",
    "                                created = True\n",
    "                                break\n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "            \n",
    "            if not tag_id:\n",
    "                # Last resort - try form data\n",
    "                for endpoint in endpoints_to_try:\n",
    "                    try:\n",
    "                        response = api_client.session.post(\n",
    "                            endpoint,\n",
    "                            data={'name': tag_name, 'color': color}\n",
    "                        )\n",
    "                        if response.status_code in [200, 201]:\n",
    "                            result = response.json()\n",
    "                            tag_id = result.get('id')\n",
    "                            if tag_id:\n",
    "                                print(f\"‚úÖ Created tag using form data\")\n",
    "                                break\n",
    "                    except:\n",
    "                        continue\n",
    "        \n",
    "        if not tag_id:\n",
    "            print(f\"‚ùå Failed to create tag. Please create '{tag_name}' manually in Tropy.\")\n",
    "            return\n",
    "        \n",
    "        # Apply tag to items\n",
    "        success_count = 0\n",
    "        failed_items = []\n",
    "        \n",
    "        for item_id in tqdm(unique_ids, desc=\"Tagging Items\"):\n",
    "            tagged = False\n",
    "            \n",
    "            # Try different endpoints for tagging\n",
    "            tag_endpoints = [\n",
    "                f\"{config.TROPY_PROJECT_API}/items/{item_id}/tags\",\n",
    "                f\"{config.TROPY_API_URL}/items/{item_id}/tags\",\n",
    "                f\"{config.TROPY_API_URL}/project/items/{item_id}/tags\"\n",
    "            ]\n",
    "            \n",
    "            for endpoint in tag_endpoints:\n",
    "                if tagged:\n",
    "                    break\n",
    "                    \n",
    "                # Try JSON payload\n",
    "                try:\n",
    "                    response = api_client.session.post(\n",
    "                        endpoint,\n",
    "                        json={\"id\": tag_id}  # Try with just id\n",
    "                    )\n",
    "                    if response.status_code in [200, 201, 204]:\n",
    "                        success_count += 1\n",
    "                        tagged = True\n",
    "                        break\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                # Try with \"tag\" wrapper\n",
    "                try:\n",
    "                    response = api_client.session.post(\n",
    "                        endpoint,\n",
    "                        json={\"tag\": tag_id}\n",
    "                    )\n",
    "                    if response.status_code in [200, 201, 204]:\n",
    "                        success_count += 1\n",
    "                        tagged = True\n",
    "                        break\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                # Try form data\n",
    "                try:\n",
    "                    response = api_client.session.post(\n",
    "                        endpoint,\n",
    "                        data={\"tag\": tag_id}\n",
    "                    )\n",
    "                    if response.status_code in [200, 201, 204]:\n",
    "                        success_count += 1\n",
    "                        tagged = True\n",
    "                        break\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            if not tagged:\n",
    "                failed_items.append(item_id)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Tagging complete!\")\n",
    "        print(f\"   Successfully tagged: {success_count} items\")\n",
    "        if failed_items:\n",
    "            print(f\"   Failed to tag: {len(failed_items)} items\")\n",
    "            print(f\"   Failed IDs: {failed_items[:5]}{'...' if len(failed_items) > 5 else ''}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during tagging: {e}\")\n",
    "        print(\"\\nüí° Troubleshooting tips:\")\n",
    "        print(\"   1. Ensure Tropy is running with REST API enabled\")\n",
    "        print(\"   2. Check if port 2019 is correct\")\n",
    "        print(\"   3. Try creating the tag manually in Tropy first\")\n",
    "\n",
    "print(\"‚úÖ Fixed tagging function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Intelligent Theme Discovery (Replace the original)\n",
    "\n",
    "def discover_semantic_themes_intelligent(n_clusters):\n",
    "    \"\"\"\n",
    "    Discover meaningful historical themes, not just document types.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüß† Discovering {n_clusters} meaningful historical themes...\")\n",
    "    \n",
    "    if embedding_matrix is None:\n",
    "        print(\"‚ùå No embeddings loaded\")\n",
    "        return\n",
    "    \n",
    "    # Run clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\n",
    "    cluster_labels = kmeans.fit_predict(embedding_matrix)\n",
    "    \n",
    "    # Words to exclude from theme names (too generic)\n",
    "    boring_words = {\n",
    "        'archival', 'document', 'page', 'visible', 'text', \n",
    "        'official', 'formal', 'written',\n",
    "        'manuscript', 'handwritten', 'typed', 'recorded', 'based',\n",
    "        'sheet', 'folio', 'recto', 'verso', 'stamp', 'seal'\n",
    "    }\n",
    "    \n",
    "    themes = []\n",
    "    \n",
    "    for i in range(n_clusters):\n",
    "        cluster_indices = np.where(cluster_labels == i)[0]\n",
    "        if len(cluster_indices) == 0: \n",
    "            continue\n",
    "        \n",
    "        # Get full summaries from this cluster\n",
    "        cluster_summaries = []\n",
    "        for idx in cluster_indices:\n",
    "            if idx < len(embeddings_records) and embeddings_records[idx].get('summary'):\n",
    "                cluster_summaries.append({\n",
    "                    'summary': embeddings_records[idx]['summary'],\n",
    "                    'item_id': embeddings_records[idx]['item_id'],\n",
    "                    'item_title': embeddings_records[idx].get('item_title', 'Untitled')\n",
    "                })\n",
    "        \n",
    "        if not cluster_summaries: \n",
    "            continue\n",
    "        \n",
    "        # Extract meaningful terms using enhanced TF-IDF\n",
    "        try:\n",
    "            # Get all text\n",
    "            texts = [item['summary'] for item in cluster_summaries]\n",
    "            \n",
    "            # Use TF-IDF with custom settings\n",
    "            vectorizer = TfidfVectorizer(\n",
    "                max_features=20,  # Get more terms to work with\n",
    "                stop_words='english',\n",
    "                ngram_range=(1, 3),  # Include phrases up to 3 words\n",
    "                min_df=2 if len(texts) > 5 else 1  # Term must appear in at least 2 docs\n",
    "            )\n",
    "            \n",
    "            tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "            feature_names = vectorizer.get_feature_names_out()\n",
    "            \n",
    "            # Get term scores for this cluster\n",
    "            scores = tfidf_matrix.sum(axis=0).A1\n",
    "            top_indices = scores.argsort()[::-1]\n",
    "            \n",
    "            # Filter out boring terms and find meaningful ones\n",
    "            meaningful_terms = []\n",
    "            entities = []\n",
    "            concepts = []\n",
    "            \n",
    "            for idx in top_indices:\n",
    "                term = feature_names[idx].lower()\n",
    "                \n",
    "                # Skip boring terms\n",
    "                if any(boring in term for boring in boring_words):\n",
    "                    continue\n",
    "                \n",
    "                # Categorize terms\n",
    "                if any(word in term for word in ['cardinal', 'bishop', 'pope', 'priest', 'sister', 'father', 'monsignor']):\n",
    "                    entities.append(term.title())\n",
    "                elif any(word in term for word in ['case', 'letter', 'request', 'petition', 'report', 'investigation']):\n",
    "                    concepts.append(term)\n",
    "                elif term[0].isupper() or any(char.isupper() for char in term[1:]):  # Likely a name or place\n",
    "                    entities.append(term.title())\n",
    "                elif len(term.split()) > 1:  # Multi-word phrases are often meaningful\n",
    "                    concepts.append(term)\n",
    "                else:\n",
    "                    meaningful_terms.append(term)\n",
    "                \n",
    "                # Stop when we have enough\n",
    "                if len(meaningful_terms) + len(entities) + len(concepts) >= 5:\n",
    "                    break\n",
    "            \n",
    "            # Build theme name prioritizing entities and concepts\n",
    "            theme_parts = []\n",
    "            if entities:\n",
    "                theme_parts.extend(entities[:2])\n",
    "            if concepts:\n",
    "                theme_parts.extend(concepts[:2])\n",
    "            if len(theme_parts) < 3 and meaningful_terms:\n",
    "                theme_parts.extend(meaningful_terms[:3-len(theme_parts)])\n",
    "            \n",
    "            if not theme_parts:  # Fallback\n",
    "                theme_parts = [t for t in feature_names[:5] if not any(b in t.lower() for b in boring_words)][:3]\n",
    "            \n",
    "            theme_name = \" + \".join(theme_parts[:3])\n",
    "            \n",
    "            # Analyze content for better description\n",
    "            # Look for patterns in summaries\n",
    "            dates = []\n",
    "            locations = []\n",
    "            topics = []\n",
    "            \n",
    "            for summary in texts[:10]:  # Analyze first 10 summaries\n",
    "                # Extract years (1900-2099)\n",
    "                import re\n",
    "                years = re.findall(r'\\b(19\\d{2}|20\\d{2})\\b', summary)\n",
    "                dates.extend(years)\n",
    "                \n",
    "                # Look for locations (capitalized words not at sentence start)\n",
    "                words = summary.split()\n",
    "                for i, word in enumerate(words[1:], 1):\n",
    "                    if word[0].isupper() and word not in boring_words:\n",
    "                        if words[i-1][-1] not in '.!?':  # Not start of sentence\n",
    "                            locations.append(word)\n",
    "            \n",
    "            # Create meaningful description\n",
    "            date_range = \"\"\n",
    "            if dates:\n",
    "                date_range = f\" ({min(dates)}-{max(dates)})\" if len(set(dates)) > 1 else f\" ({dates[0]})\"\n",
    "            \n",
    "            # Get unique item IDs\n",
    "            item_ids = list(set([item['item_id'] for item in cluster_summaries]))\n",
    "            \n",
    "            themes.append({\n",
    "                'id': len(themes) + 1,\n",
    "                'name': theme_name + date_range,\n",
    "                'photo_count': len(cluster_indices),\n",
    "                'item_count': len(item_ids),\n",
    "                'item_ids': item_ids,\n",
    "                'summaries': cluster_summaries[:5],  # Keep full summary objects\n",
    "                'key_entities': entities[:3],\n",
    "                'key_concepts': concepts[:3],\n",
    "                'date_range': date_range\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing cluster {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not themes:\n",
    "        print(\"‚ùå No themes could be extracted.\")\n",
    "        return\n",
    "    \n",
    "    # Display themes with rich information\n",
    "    print(f\"\\nüìä Discovered {len(themes)} Historical Themes:\\n\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    for theme in themes:\n",
    "        print(f\"\\nüè∑Ô∏è  THEME {theme['id']}: {theme['name']}\")\n",
    "        print(f\"   üìö Scope: {theme['photo_count']} photos across {theme['item_count']} items\")\n",
    "        \n",
    "        if theme['key_entities']:\n",
    "            print(f\"   üë• Key People/Places: {', '.join(theme['key_entities'])}\")\n",
    "        if theme['key_concepts']:\n",
    "            print(f\"   üìã Key Topics: {', '.join(theme['key_concepts'])}\")\n",
    "        \n",
    "        print(f\"\\n   üìñ Representative Documents:\")\n",
    "        for j, item in enumerate(theme['summaries'][:3], 1):\n",
    "            print(f\"\\n   [{j}] Item: {item['item_title']}\")\n",
    "            # Show meaningful excerpt (skip generic opening)\n",
    "            summary = item['summary']\n",
    "            # Try to find the most interesting part\n",
    "            sentences = summary.split('. ')\n",
    "            interesting_part = summary\n",
    "            for sentence in sentences:\n",
    "                if any(term in sentence.lower() for term in theme['key_entities'] + theme['key_concepts']):\n",
    "                    interesting_part = sentence\n",
    "                    break\n",
    "            print(f\"       \\\"{interesting_part[:200]}...\\\"\")\n",
    "        \n",
    "        print(\"\\n\" + \"-\" * 100)\n",
    "    \n",
    "    # Interactive selection\n",
    "    print(\"\\nüéØ Select themes to tag (more meaningful tags will be created):\")\n",
    "    print(\"   ‚Ä¢ Enter theme numbers (e.g., 1,3,5)\")\n",
    "    print(\"   ‚Ä¢ Type 'all' for all themes\")\n",
    "    print(\"   ‚Ä¢ Type 'details X' to see more about theme X\")\n",
    "    print(\"   ‚Ä¢ Press Enter to skip\")\n",
    "    \n",
    "    while True:\n",
    "        choice = input(\"\\nYour selection: \").strip().lower()\n",
    "        \n",
    "        if choice == '':\n",
    "            print(\"Skipping tagging.\")\n",
    "            return\n",
    "        \n",
    "        if choice.startswith('details '):\n",
    "            try:\n",
    "                theme_num = int(choice.split()[1])\n",
    "                theme = next((t for t in themes if t['id'] == theme_num), None)\n",
    "                if theme:\n",
    "                    print(f\"\\nüìö Full details for Theme {theme_num}:\")\n",
    "                    for j, item in enumerate(theme['summaries'], 1):\n",
    "                        print(f\"\\n[{j}] {item['item_title']}\")\n",
    "                        print(f\"    {item['summary']}\\n\")\n",
    "                else:\n",
    "                    print(\"Theme not found.\")\n",
    "            except:\n",
    "                print(\"Use format: details 3\")\n",
    "            continue\n",
    "        \n",
    "        break\n",
    "    \n",
    "    # Parse selection and create meaningful tags\n",
    "    if choice == 'all':\n",
    "        selected_themes = themes\n",
    "    else:\n",
    "        try:\n",
    "            selected_indices = [int(x.strip()) for x in choice.split(',')]\n",
    "            selected_themes = [t for t in themes if t['id'] in selected_indices]\n",
    "        except:\n",
    "            print(\"‚ùå Invalid selection.\")\n",
    "            return\n",
    "    \n",
    "    # Apply meaningful tags\n",
    "    print(f\"\\nüè∑Ô∏è  Creating {len(selected_themes)} meaningful tags...\")\n",
    "    \n",
    "    for theme in selected_themes:\n",
    "        # Create a more specific tag name\n",
    "        if theme['key_entities'] and theme['key_concepts']:\n",
    "            tag_name = f\"{theme['key_concepts'][0].title()}: {theme['key_entities'][0]}{theme['date_range']}\"\n",
    "        elif theme['key_concepts']:\n",
    "            tag_name = f\"{theme['key_concepts'][0].title()}{theme['date_range']}\"\n",
    "        else:\n",
    "            tag_name = f\"Theme: {theme['name']}\"\n",
    "        \n",
    "        print(f\"\\n   Creating tag: {tag_name}\")\n",
    "        add_tag_to_items_fixed(theme['item_ids'], tag_name)\n",
    "\n",
    "print(\"‚úÖ Intelligent clustering ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Fixed Metadata Enhancement Function (Corrected Version)\n",
    "\n",
    "def enhance_item_titles():\n",
    "    \"\"\"Update alternative titles (dcterms:alternative) with AI suggestions using correct Tropy API.\"\"\"\n",
    "    if not item_summaries: \n",
    "        print(\"‚ùå No item summaries loaded to enhance titles.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"‚úçÔ∏è Preparing to update machine-generated titles for {len(item_summaries)} items...\")\n",
    "    print(\"   This will populate the 'dcterms:alternative' field (Machine-generated title)\")\n",
    "    \n",
    "    if input(\"\\nContinue? (y/n): \").lower() != 'y':\n",
    "        print(\"Cancelled.\")\n",
    "        return\n",
    "    \n",
    "    success_count = 0\n",
    "    failed_count = 0\n",
    "    skipped_count = 0\n",
    "    \n",
    "    # First, let's verify we can reach the API\n",
    "    try:\n",
    "        test_response = api_client.session.get(f\"{config.TROPY_PROJECT_API}/items\")\n",
    "        if test_response.status_code != 200:\n",
    "            print(f\"‚ùå Cannot connect to Tropy API. Status: {test_response.status_code}\")\n",
    "            return\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå API connection failed: {e}\")\n",
    "        return\n",
    "    \n",
    "    for summary_data in tqdm(item_summaries, desc=\"Updating Alternative Titles\"):\n",
    "        suggested_title = summary_data.get('suggested_title')\n",
    "        \n",
    "        if not suggested_title:\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "            \n",
    "        item_id = summary_data['item_id']\n",
    "        \n",
    "        # Correct payload format as specified by developer\n",
    "        payload = {\n",
    "            \"http://purl.org/dc/terms/alternative\": suggested_title\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Use the correct endpoint: /project/data/{id}\n",
    "            response = api_client.session.post(\n",
    "                f\"{config.TROPY_API_URL}/project/data/{item_id}\", \n",
    "                json=payload\n",
    "            )\n",
    "            \n",
    "            if response.status_code in [200, 201, 204]:\n",
    "                success_count += 1\n",
    "            else:\n",
    "                # Log error details for debugging\n",
    "                if response.status_code == 404:\n",
    "                    print(f\"  ‚ö†Ô∏è  Item {item_id} not found in project\")\n",
    "                else:\n",
    "                    print(f\"  ‚ùå Failed for item {item_id}: Status {response.status_code}\")\n",
    "                    if response.text:\n",
    "                        print(f\"     Response: {response.text[:200]}\")\n",
    "                failed_count += 1\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to update item {item_id}: {e}\")\n",
    "            failed_count += 1\n",
    "    \n",
    "    print(f\"\\n‚úÖ Title enhancement complete!\")\n",
    "    print(f\"   Successfully updated: {success_count} items\")\n",
    "    print(f\"   Skipped (no suggestion): {skipped_count} items\")\n",
    "    if failed_count > 0:\n",
    "        print(f\"   Failed: {failed_count} items\")\n",
    "    \n",
    "    print(f\"\\nüí° Check your Tropy items - the 'Machine-generated title' field should now be populated!\")\n",
    "\n",
    "print(\"‚úÖ Metadata enhancement function ready\")\n",
    "\n",
    "# Improved function to check item fields and validate structure\n",
    "def check_item_fields():\n",
    "    \"\"\"Check what fields are available in Tropy items and validate API connectivity.\"\"\"\n",
    "    print(\"\\nüîç Checking available fields in Tropy items...\")\n",
    "    \n",
    "    try:\n",
    "        # Get items list\n",
    "        response = api_client.session.get(f\"{config.TROPY_PROJECT_API}/items\")\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Failed to get items. Status: {response.status_code}\")\n",
    "            print(f\"Response: {response.text}\")\n",
    "            return\n",
    "            \n",
    "        items = response.json()\n",
    "        \n",
    "        if not items or len(items) == 0:\n",
    "            print(\"‚ùå No items found in the project\")\n",
    "            return\n",
    "            \n",
    "        first_item_id = items[0].get('id')\n",
    "        print(f\"üìã Checking structure of item ID: {first_item_id}\")\n",
    "        \n",
    "        # Check current metadata using the data endpoint\n",
    "        data_response = api_client.session.get(f\"{config.TROPY_API_URL}/project/data/{first_item_id}\")\n",
    "        \n",
    "        if data_response.status_code == 200:\n",
    "            current_data = data_response.json()\n",
    "            print(f\"\\nüìã Current metadata for item {first_item_id}:\")\n",
    "            \n",
    "            if current_data:\n",
    "                for field, value in current_data.items():\n",
    "                    short_value = str(value)[:50] + \"...\" if len(str(value)) > 50 else str(value)\n",
    "                    print(f\"   - {field}: {short_value}\")\n",
    "                    \n",
    "                # Check for dcterms:alternative specifically\n",
    "                if 'http://purl.org/dc/terms/alternative' in current_data:\n",
    "                    print(\"\\n‚úÖ Found dcterms:alternative field!\")\n",
    "                else:\n",
    "                    print(\"\\n‚ö†Ô∏è  dcterms:alternative field not found in current metadata.\")\n",
    "                    print(\"   This might be normal if no alternative title has been set yet.\")\n",
    "            else:\n",
    "                print(\"   No metadata found for this item\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Could not get metadata. Status: {data_response.status_code}\")\n",
    "            print(\"   This is normal if the item has no metadata yet.\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error checking fields: {e}\")\n",
    "\n",
    "# Additional helper function to test a single item update\n",
    "def test_single_item_update():\n",
    "    \"\"\"Test updating a single item to debug the process.\"\"\"\n",
    "    if not item_summaries:\n",
    "        print(\"‚ùå No item summaries loaded.\")\n",
    "        return\n",
    "    \n",
    "    # Get first item with a suggested title\n",
    "    test_item = None\n",
    "    for summary in item_summaries:\n",
    "        if summary.get('suggested_title'):\n",
    "            test_item = summary\n",
    "            break\n",
    "    \n",
    "    if not test_item:\n",
    "        print(\"‚ùå No items with suggested titles found.\")\n",
    "        return\n",
    "    \n",
    "    item_id = test_item['item_id']\n",
    "    suggested_title = test_item['suggested_title']\n",
    "    \n",
    "    print(f\"üß™ Testing update for item {item_id}\")\n",
    "    print(f\"   Suggested title: '{suggested_title}'\")\n",
    "    \n",
    "    payload = {\n",
    "        \"http://purl.org/dc/terms/alternative\": suggested_title\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # First, check current data\n",
    "        current_response = api_client.session.get(f\"{config.TROPY_API_URL}/project/data/{item_id}\")\n",
    "        print(f\"\\n   Current data status: {current_response.status_code}\")\n",
    "        \n",
    "        if current_response.status_code == 200:\n",
    "            current_data = current_response.json()\n",
    "            if current_data:\n",
    "                print(\"   Current metadata fields:\")\n",
    "                for field in current_data.keys():\n",
    "                    print(f\"     - {field}\")\n",
    "        \n",
    "        # Now try to update\n",
    "        print(f\"\\n   Attempting update to endpoint: {config.TROPY_API_URL}/project/data/{item_id}\")\n",
    "        print(f\"   Payload: {payload}\")\n",
    "        \n",
    "        response = api_client.session.post(\n",
    "            f\"{config.TROPY_API_URL}/project/data/{item_id}\", \n",
    "            json=payload,\n",
    "            headers={'Content-Type': 'application/json'}\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n   Update response status: {response.status_code}\")\n",
    "        if response.text:\n",
    "            print(f\"   Update response text: {response.text[:200]}\")\n",
    "        \n",
    "        if response.status_code in [200, 201, 204]:\n",
    "            print(\"\\n‚úÖ Test update successful!\")\n",
    "            \n",
    "            # Verify the update worked\n",
    "            verify_response = api_client.session.get(f\"{config.TROPY_API_URL}/project/data/{item_id}\")\n",
    "            if verify_response.status_code == 200:\n",
    "                updated_data = verify_response.json()\n",
    "                if 'http://purl.org/dc/terms/alternative' in updated_data:\n",
    "                    print(f\"   ‚úÖ Confirmed: Alternative title is now: '{updated_data['http://purl.org/dc/terms/alternative']}'\")\n",
    "                else:\n",
    "                    print(\"   ‚ö†Ô∏è  Alternative title field not found after update\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"\\n‚ùå Test update failed!\")\n",
    "            print(\"   Troubleshooting tips:\")\n",
    "            print(\"   1. Check if the item ID exists in Tropy\")\n",
    "            print(\"   2. Ensure Tropy is running with REST API enabled\")\n",
    "            print(\"   3. Verify the dcterms:alternative field is in your template\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Test failed with exception: {e}\")\n",
    "\n",
    "# Batch update with better error handling\n",
    "def batch_update_titles(batch_size=10):\n",
    "    \"\"\"Update titles in batches with detailed error reporting.\"\"\"\n",
    "    if not item_summaries:\n",
    "        print(\"‚ùå No item summaries loaded.\")\n",
    "        return\n",
    "    \n",
    "    items_with_titles = [s for s in item_summaries if s.get('suggested_title')]\n",
    "    \n",
    "    if not items_with_titles:\n",
    "        print(\"‚ùå No items with suggested titles found.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üìä Found {len(items_with_titles)} items with suggested titles\")\n",
    "    \n",
    "    total_batches = (len(items_with_titles) + batch_size - 1) // batch_size\n",
    "    \n",
    "    success_count = 0\n",
    "    failed_items = []\n",
    "    \n",
    "    for batch_num in range(total_batches):\n",
    "        start_idx = batch_num * batch_size\n",
    "        end_idx = min(start_idx + batch_size, len(items_with_titles))\n",
    "        batch = items_with_titles[start_idx:end_idx]\n",
    "        \n",
    "        print(f\"\\nüì¶ Processing batch {batch_num + 1}/{total_batches} ({len(batch)} items)...\")\n",
    "        \n",
    "        for item in batch:\n",
    "            item_id = item['item_id']\n",
    "            suggested_title = item['suggested_title']\n",
    "            \n",
    "            payload = {\n",
    "                \"http://purl.org/dc/terms/alternative\": suggested_title\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                response = api_client.session.post(\n",
    "                    f\"{config.TROPY_API_URL}/project/data/{item_id}\", \n",
    "                    json=payload\n",
    "                )\n",
    "                \n",
    "                if response.status_code in [200, 201, 204]:\n",
    "                    success_count += 1\n",
    "                    print(f\"   ‚úÖ {item_id}\")\n",
    "                else:\n",
    "                    failed_items.append({\n",
    "                        'id': item_id,\n",
    "                        'status': response.status_code,\n",
    "                        'error': response.text[:100] if response.text else 'No error message'\n",
    "                    })\n",
    "                    print(f\"   ‚ùå {item_id} (Status: {response.status_code})\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                failed_items.append({\n",
    "                    'id': item_id,\n",
    "                    'error': str(e)\n",
    "                })\n",
    "                print(f\"   ‚ùå {item_id} (Exception: {str(e)[:50]}...)\")\n",
    "        \n",
    "        # Small delay between batches\n",
    "        if batch_num < total_batches - 1:\n",
    "            time.sleep(0.5)\n",
    "    \n",
    "    print(f\"\\nüìä Final Results:\")\n",
    "    print(f\"   ‚úÖ Success: {success_count}/{len(items_with_titles)}\")\n",
    "    print(f\"   ‚ùå Failed: {len(failed_items)}\")\n",
    "    \n",
    "    if failed_items:\n",
    "        print(\"\\n‚ùå Failed items details:\")\n",
    "        for item in failed_items[:5]:  # Show first 5\n",
    "            print(f\"   - ID: {item['id']}\")\n",
    "            if 'status' in item:\n",
    "                print(f\"     Status: {item['status']}\")\n",
    "            print(f\"     Error: {item.get('error', 'Unknown error')}\")\n",
    "        \n",
    "        if len(failed_items) > 5:\n",
    "            print(f\"   ... and {len(failed_items) - 5} more\")\n",
    "\n",
    "print(\"‚úÖ Enhanced metadata functions ready\")\n",
    "print(\"\\nüí° Available functions:\")\n",
    "print(\"   check_item_fields()      - Verify API connectivity and field availability\")\n",
    "print(\"   test_single_item_update() - Test with one item first\")\n",
    "print(\"   enhance_item_titles()     - Update all titles\")\n",
    "print(\"   batch_update_titles(20)   - Update in batches with size control\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Interactive Main Menu (Complete version)\n",
    "def main_menu():\n",
    "    \"\"\"Enhanced main menu with better search integration.\"\"\"\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"   TROPY AI ANALYSIS & TAGGING WORKBENCH\")\n",
    "        print(f\"   Model: {selected_model.upper()}\")\n",
    "        print(\"=\"*50)\n",
    "        print(\"1. üîé Search & Tag (Ask a question)\")\n",
    "        print(\"2. üß† Discover Semantic Themes (Clustering)\")\n",
    "        print(\"3. ‚úçÔ∏è Enhance Metadata with AI Titles\")\n",
    "        print(\"4. üö™ Exit\")\n",
    "        \n",
    "        choice = input(\"\\nPlease select an option (1-4): \").strip()\n",
    "        \n",
    "        if choice == '1':\n",
    "            search_and_tag_workflow()\n",
    "        \n",
    "        elif choice == '2':\n",
    "            try:\n",
    "                num_clusters = int(input(\"How many themes to discover? (e.g., 8-15 is a good start): \").strip())\n",
    "                if num_clusters > 1:\n",
    "                    if 'discover_semantic_themes_intelligent' in globals():\n",
    "                        discover_semantic_themes_intelligent(num_clusters) # doesn't exist! ST\n",
    "                    else:\n",
    "                        discover_semantic_themes(num_clusters)\n",
    "            except ValueError:\n",
    "                print(\"Invalid number. Please enter an integer.\")\n",
    "\n",
    "        elif choice == '3':\n",
    "            enhance_item_titles()\n",
    "            \n",
    "        elif choice == '4':\n",
    "            print(\"Exiting. Goodbye!\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid choice. Please try again.\")\n",
    "\n",
    "# Display collection info\n",
    "if embeddings_records:\n",
    "    print(f\"\\nüìä Collection size: {len(embeddings_records)} photos\")\n",
    "    print(\"‚ú® Ready for analysis!\")\n",
    "    \n",
    "    # Quick stats about the collection\n",
    "    unique_items = len(set(r.get('item_id') for r in embeddings_records))\n",
    "    print(f\"üìö Unique items: {unique_items}\")\n",
    "    \n",
    "    # Check if summaries exist\n",
    "    summaries_count = sum(1 for r in embeddings_records if r.get('summary'))\n",
    "    print(f\"üìù Photos with summaries: {summaries_count}/{len(embeddings_records)}\")\n",
    "\n",
    "# Run the main menu\n",
    "main_menu()"
   ]
  }
 ],
   "metadata": {
    "kernelspec": {
     "display_name": "Python 3 (ipykernel)",
     "language": "python",
     "name": "python3"
    },
    "language_info": {
     "codemirror_mode": {
      "name": "ipython",
      "version": 3
     },
     "file_extension": ".py",
     "mimetype": "text/x-python",
     "name": "python",
     "nbconvert_exporter": "python",
     "pygments_lexer": "ipython3",
     "version": "3.13.2"
    }
   },
   "nbformat": 4,
   "nbformat_minor": 4
  }
